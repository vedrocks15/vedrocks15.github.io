<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Vedant S. Joshi</title>
  
  <meta name="author" content="Vedant S. Joshi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vedant S. Joshi</name>
              </p>
              <p>Hi, I am a <b>second-year Master's student</b> in Computer Science and Engineering department at UC San Diego. 
                For the summer of 2024, I was interning with the Video Engineering Team at <b>Apple</b>, under <a href = "https://scholar.google.com/citations?user=07NrZFIAAAAJ&hl=en">Dr. Javier Movellan</a>.
                My work was focused on enhancing data mixtures for pre training of grounded multi-modal large language models (VLMs) & exploring 
                the multi-modal token space of such VLMs.
                <br>
                <br>
                
                I have had professional experience as a Vision & Imaging Engineer at <a href="https://tonboimaging.com/consumer/">Tonbo Imaging</a>, where my role 
                was to build robust vision models that improved the spatial awareness of automobiles on Indian roads. My models covered the domain of object localization 
                during day & night (using feature fusion strategies between RGB - IR domain), self-supervised depth map estimation & open world object detection.
                                
                <br>
                <br>

                <p style="color:red;">Currently I am open to full time roles in computer vision, multi-modal & LLM research roles for March 2025.</p>

                <br>

        

                <!-- Previously, I was a Research Data Scientist at <a href="https://www.vedantu.com">Vedantu</a>, where my job was to build label efficient A.I. solutions. 
                My work covered the domain of vision, natural language & user centric tabular data originating in 
                the EdTech sector. The goal of my research at Vedantu under <a href="https://scholar.google.com/citations?user=ZFDbLisAAAAJ&hl=en">Dr. Sivanagaraja Tatinati</a> was to 
                leverage the true power of self-supervision to build more accurate models that cater to the needs of students & company stakeholders.  -->


              
              </p>

              <p style="text-align:center">
                <a href="mailto:vedrocks15@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/vedantPhdCV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/Resume_vedant_joshi.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://github.com/vedrocks15/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/vedant-joshi-b822bb169/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://www.kaggle.com/vedantj/code/">Kaggle</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/vedant.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/vedant.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <br>
        
        
      
        <hr style="width:50%">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                <b>How to make machines percieve the world ? </b> <br>
                Is the question that drives my research every morning. 
                Based on this question, my fundamental goal is to understand the inner workings of vision language models & make a sincere contribution 
                towards building dynamic as well as modular mechanisms that match the level of human intelligence by efficiently 
                representing a multitude of input modalities into a single structured latent space & are contextually modifiable based on the situation at hand.
                
                
                <br>
                <br>
                <b>Areas of Interest :</b>
              <ul>
                <li>Representational learning</li>
                <li>Multimodal LLMs (Vision Language models)</li>
                <li>Self-supervised learning</li>
                <li>Multimodal latent space explainability</li>
                <li>Open World Learning</li>
                <li>Reflectance Models</li>
                <li>Model quantization & pruning</li>
                <li>Pseudo labelled data generation</li>
              </ul>

              </p>
            </td>
          </tr>
        </tbody></table>
        <hr style="width:50%">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Attention Splat</heading><br>
                <subheading>3d Scene Editing</subheading>
              <br>
              <br>
              <strong>Vedant S. Joshi</strong>,
              <a href="https://cseweb.ucsd.edu/~mkchandraker/">Prof. Manmohan Chandrekar</a> | 
              <a href="https://drive.google.com/file/d/1-i93HD7-0GWLZy2zp6YIqJwnZG-H6eoR/view">Project</a>, 2024
              <br>
              
              <p>
                3D Gaussian Splatting (3DGS) represents the state-of-the-art in 3D scene reconstruction, functioning by 
                projecting 3D Gaussians onto 2D camera planes and rasterizing images. Recently, 3DGS has been adapted to various tasks
                beyond simple 3D scene reconstruction, tasks that were previously explored using Neural Radiance Fields (NeRFs).
                These tasks include 3D object segmentation, scene relighting, and modeling in-the-wild scenes, often requiring the
                addition of feature vectors to points in 3D space, as represented implicitly by NeRFs. Recent work has focused on
                attaching extra feature vectors to these Gaussians as optimization parameters to hold additional information. 
              </p>
              <img src="images/demo.gif" alt="paper diagrams" style="width:100%;max-width:100%">
              <p>
                However, the optimization of these features occurs in the 2D image space after rasterization. While this approach allows
                faithful updating and optimization of features, we believe it lacks 3D awareness. Our goal is to improve the quality of feature vectors 
                learnt in each gaussian by making them aware of their local & global neighbourhood. We propose contextualization of vectors 
                via self-attention in the explicit 3D representation via local & global transformers. Our initial experiments highlight several 
                implementation challenges associated with introducing the computationally expensive O(n2) 
                self-attention mechanism. We provide solutions to address these challenges and demonstrate the effectiveness of our approach.
              </p>
            </td>	
          </tr>	
        </tbody></table>

        <hr style="width:50%">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>YZR-Net</heading><br>
                <subheading><b>S</b>elf-supervised <b>H</b>idden representations <b>I</b>nvariant to <b>T</b>ransformations for profanity detection <b>[S.H.I.T]</b></subheading>
              <br>
              <br>
              <strong>Vedant S. Joshi</strong>,
              <a href="https://scholar.google.com/citations?user=ZFDbLisAAAAJ&hl=en">Dr. Sivanagaraja Tatinati</a>,
							<a href="https://scholar.google.co.in/citations?user=bXXM7rMAAAAJ&hl=en">Dr. Yubo Wang</a> | 
              <a href="https://arxiv.org/abs/2211.15532">PrePrint arXiv</a>, 2022
              <br>
              
              <p>
                In this new age of online learning students are still getting accustomed to virtual sessions. Watching recorded videos in the name of attending classes seems 
                to be the trend followed by many EdTech players but at <b>Vedantu</b>, we understand the problems associated with such passive setups. Therefore we provide live classes 
                where students can directly chat with their teachers as well as peers & enjoy the simulated experience of offline classes in an online world. The chatting framework is 
                crucial to our platform for achieving an improved flow of information between the players involved. Sometimes certain <b>miscreants</b> in the class, 
                use our chats framework to post certain insults or abusive language to disturb the decorum of the session. Posting ill intent messages that are directed 
                towards another student, teacher, racial group or gender can foster the feeling of negativity in the mind of the receiver. Even though such situations 
                occur rarely, they must be addressed instantaneously due to the magnitude of their impact on a student‚Äôs mind.
              </p>

              <img src="images/profanity.png" alt="paper diagrams" style="width:100%;max-width:100%">
              <p>
                Since students don't strictly adhere to syntax or grammatical rules in a chat environment, our solution needs to be invaraint to noisy substitues to a given word. 
                Also there are scenarios where some creative players come-up with clever work arounds such as <b>self-censoring (f**k) or random character deletions (fck)</b> to fool
                our detection system & still maintain the negative intent of the chat being posted on our platform.

                <b>YZR-Net</b> is our NLP implementation of the image SSL framework <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>. It is trained on a
                multi-pair, instance discrimination objective for words & since we are using a dictionary of unique words we are 100% sure that the negative pairs being generated, 
                are semantically
                separate in nature.

                For our NLP usecase the whole framework seemed ideal because we had an added challenge of profanity detection in the unstructured, 
                transliterated language <b>Hinglish</b>. Due to lack of research or pre-trained models in Hinglish, we had to formulate our learning objective on word syntax instead of 
                learning the loosely defined semantics of the language. Our final goal was to learn a structured latent space where representations
                of profane words & their augmented counterparts are present in close proximity ( <b>metric : cosine similarity </b>).
              </p>

              <p>
                Our system improved the baseline regex <b>recall by 10%</b> & the precision was maintained by using a false positive (<b>similar in syntax but different in semantics</b>) 
                dictionary of non profane words which are removed in the pre-processing pipeline. The key achievement of our model is that now we have a compact dictionary of key profane tokens only & this dictionary can be 
                updated without retraining the YZR-Net incase a new profane token is coined by our creative students ! The implementation is useful only when there is a profane token used 
                in the chat & therefore as a part of version 2, we are working on <b>Heirarchial Atttention Networks</b> to capture cases where a complex combination of non profane tokens 
                sends an ill intent message.
              </p>

            </td>	
          </tr>	
        </tbody></table>	
        
        <hr style="width:50%">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Looking For A Match</heading><br>
                <subheading>Self-supervised Clustering For Automatic Doubt Matching In e-learning Platforms</subheading>
              <br>
              <br>
              <strong>Vedant S. Joshi</strong>,
              <a href="https://scholar.google.com/citations?user=ZFDbLisAAAAJ&hl=en">Dr. Sivanagaraja Tatinati</a>,
							<a href="https://scholar.google.co.in/citations?user=bXXM7rMAAAAJ&hl=en">Dr. Yubo Wang</a> | 
              <a href="https://arxiv.org/abs/2208.09600">PrePrint arXiv</a>, 2022
              <br>
              
              <p>
                Doubts are a natural outcome of any student's learning journey & solving them immediately becomes imperative for any EdTech platform so that the steady
                growth of learning for every child can be maintained. In this era of <b>Big Data</b> the volume of data being generated on Vedantu is huge & this is 
                also applicable for the doubts being asked on our platform. The traditional way of solving doubts by <b>Subject Matter Experts</b>(SME) is time consuming, redundant
                & infeasible in nature. Assigning one SME per doubt for every student is unimaginable. Therefore coming up with a system that can find a possible solved match for an asked doubt & create clusters of semantically similar doubts so that
                the redundancy in answering the same question by SMEs is reduced, would help our platform immensely.    
              </p>

              <img src="images/paperDiags.png" alt="paper diagrams" style="width:100%;max-width:100%">


              <p>
                In this work we solved a sub-problem of diagram based matching for doubt questions. For the non-diagram questions we rely on the power of OCRs & transformers
                to build strong text matching engines but for pure <b>reverse image search</b> we come-up with a diagram extraction & matching module which allowed us to prevent 
                re-answering of the same question & reduced the redundancy of diagram based doubts from <b>9.5 lakh individual points</b> to <b>2.5 lakh clusters</b> 
                in our database. Our solution utilised the SOTA, self supervised framework BYOL in this project because there are a lot of similar diagram images in our training set & utilising
                the <b>instance discrimination</b> objective in <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> & <a href="https://arxiv.org/abs/1911.05722">MoCo</a> for 
                negative pairs had a high chance of pushing away the latent representations of 2 semantically relevant diagram images thereby affecting our top-5 matching scores.
              </p>
            </td>	
          </tr>	
          <tr>
            <td style="padding:20px;width:100%;text-align:left;">
              <image src="images/paperResults.png" style="float:right;width:50%;max-width:50%" alt="paper res">
                <p>
                  Our implementation is an improved version of the 
                  <a href="https://arxiv.org/abs/2006.07733">BYOL</a> which is trained on diagram images extracted from a 
                  custom trained <a href="https://arxiv.org/abs/2011.08036">Scaled YOLOv4</a> module that gave a mean average 
                  precision of <b>90%</b> for diagram detection. The main contribution of our work was to come up with a new, domain specific augmentation
                  pipeline that modelled the possible noises generated by a student while uploading an image on our platform. The newly designed augmentations were
                  guided by <b>mutual information</b> metrics to capture flashes, skews, random camera noise etc. but at the same time not loose semantic relevance of the 
                  matching pairs. The augmentations played a crucial role 
                  towards learning of noise invariant, compressed representations of diagrams & aided in achieving more accurate as well as relevant matches for a given 
                  input query.
                </p>
                <p>
                  The diagrams on the right show the improved convergance of our models with stronger clustering abilities along the diagonals of <b>image 
                  similarity matrices</b>. In order to deploy the model, the vectors computed by our <b>Custom BYOL</b> were searched using approximate K-NN algorithms such 
                  as <a href="https://arxiv.org/abs/1603.09320">Heirarchial Navigable Small Worlds (HNSW)</a> & the search space was reduced by 
                  performing clustering on the vectors using <a href="https://arxiv.org/abs/1802.03426">UMAP</a> & <a href="https://arxiv.org/abs/1911.02282">HDBSCAN</a>. Each
                  cluster was represented by a centroid which was the key vector in the HNSW search space.
                </p>
            </td>
          </tr>
        </tbody></table>	
        
        <hr style="width:50%">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>For Your Eyes Only</heading><br>
                <subheading>Character Level Model For Lip Reading [
                                                                    <a href = "https://mgmits.ac.in/icscc-2020/">
                                                                      <b style='color:red;'>Won Best Paper</b>
                                                                    </a>]</subheading>
              <br>
              <br>
              <strong>Vedant S. Joshi</strong>,
              <a href="https://www.iiitkottayam.ac.in/#!/faculty/ebinn">Dr. Ebin Deni Raj</a> | 
              <a href="https://ieeexplore.ieee.org/document/9528104">2021 8th International Conference on Smart Computing & Communications (ICSCC)</a>
              <br>
              
              <p>
                The amazing ability of our human mind to handle multiple input sources at once & make sense of the environment in which we are present is truly
                extra-ordinary. Along with this ability, our mind is also dynamic enough to adapt to situations where we loose certain input sources & still make the best
                possible use of the information availale to us. We all think that speech understanding is a skill that is entirely dependent on hearing but vision also 
                plays a key hidden role which helps us to disambiguate a lot of confusing scenarios. For people who have trouble in hearing, they rely a lot on their vision to
                understand speech. Based on this observation, I worked on my thesis titled <b> For Your Eyes Only </b>, to build an end to end deep learning 
                system which is able to accurately map a set of lip movements in a given video to its corresponding character.
              </p>
              </td>
              </tr>

              <tr>
                <td style="padding:20px;width:100%;text-align:right;vertical-align: middle;">
                  <image src="images/lipNet.png" style="float:left;width:50%;max-width:50%" alt="left aligned image" class="hoverZoomLink">
                  <p>
                    The system is trained on a subset of words from the large scale <a href = "https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html">Oxford-BBC Lip Reading in the Wild (LRW) Dataset</a>.
                    The whole problem statement is formulated around single word prediction, character by character because the aim of our thesis was to come up with a system that
                    could learn lip movement to character mapping using only a limited set of words in training. This approach of <b>character level prediction</b>
                    made our system more generalized in nature, incase we encountered a previously unseen word. The out of vocabulary words cannot be handled by simple classification models.
                  </p>
                  <p>
                    We train the model on <b>112 words</b> & the videos for each of them are processed according to the timestamp at which they are spoken. To further 
                    ease the task of learning in <b>spatial domain</b>, we make use of <a href = "https://pypi.org/project/dlib/">D-Lib facial features library</a> to extract only the 
                    lip region from these videos with some buffer. Also speaking speed normalisation is applied so that every video would be 22 frames long in 
                    the <b>temporal dimension</b> & support efficient batching of training data into tensors.
                  </p>

                  <p>
                    For baseline performance we repurposed <a href = "https://arxiv.org/abs/1611.01599">DeepMind's LipNet model</a> from sentence level to word level predictor & train 
                    it from scratch on the LRW subset. To learn complex frame interactions & improve model explainability, we added 
                    <a href="https://arxiv.org/abs/1409.0473">Bahdanau attention mechanism</a> in between the the 3D ConvNet encoder & GRU decoder which 
                    is depicted by the heatmap on the left. The final prediction is made by using <b>greedy CTC decoding techniques</b> in order to get a single character that was 
                     scaled to multiple frames during the alignment learning. 
                    <br>
                    <br>
                    <b>Result shown below.</b>
                  </p>
                </td>
              </tr>	


              <tr>
                <td>
                  <image src="data/lipReadingVid.gif" style="float:right;width:100%;max-width:100%" alt="right aligned image" class="hoverZoomLink">
                </td>
              </tr>	

        </tbody></table>	
        <br>
        <hr style="width:50%">
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Quantized Coconut Detection Models with  Edge Devices</heading>
              <br>
              <br>
              <strong>Vedant S. Joshi</strong>,
              <a href="https://www.iiitkottayam.ac.in/#!/faculty/ebinn">Dr. Ebin Deni Raj</a>,
							<a href="https://scholar.google.com/citations?user=1G2WBksAAAAJ&hl=en">Jeena Thomas</a> | 
              <a href="https://www.worldscientific.com/doi/abs/10.1142/S0219265921440102">Journal of Interconnection Networks</a>, 2022        
            </td>		
          </tr>	
          
          <tr>
            <td style="padding:20px;width:100%;text-align:left;vertical-align: middle;">
              <image src="data/cocoLayersModel.png" style="float:right;width:50%;max-width:50%" alt="right aligned image" class="hoverZoomLink">
              <p>
              Owing to the landscape & natural conditions, <b>coconut</b> is an important fruit in Kerala. Coconut farms are an important aspect
              of the state's economy therefore a lot of citizen's livelihood is dependent on this fruit. Plucking coconuts from plam trees is a huge 
              challenge since it involves scaling large heights & without proper equipment, it could lead to serious injuries for the daily wage
              workers. Inspired by this, I started to work on my Honours degree thesis project <b>Coco-Layers</b>.
              </p>
              <p>
                The whole project was divided into 2 modules : 
                <ul>
                  <li><b>Hardware :</b> Building of an autonomous drone with flight stabilization for curation of a novel
                    coconut dataset which captured variances in real world lighting conditions, multiple scales at which coconuts occur in wild & 
                    possible hindrances that could confuse the overall detection system.</li>
                    <br>
                  <li><b>Software : </b> On board object detecion module that supported edge based real time inference without loosing detection accuracy.</li>
                </ul>

              We experimented on <b>Nvidia Jetson Nano</b> & <b>Raspberry Pi 3b+</b> using <b>Nvidia tensor RT</b> and <b>Tensorflow lite quantisation</b> kits for reducing 
              model resource consumption. To further push the limits, a frame buffer handling mechanism was written in OpenCV to reduce the calls to main memory for achieving faster frame
              processing rates. The peak accuracy achieved in the study was <b>0.4 mean average precision</b> with a <b>22 FPS</b> detection rate 
              (using a <a href="https://arxiv.org/abs/2004.10934">Tiny YOLOv4</a>) in real time flight. The downside
              of fast processing was reduced flight time from <b>12 minutes to 6 minutes</b> which was kept as a goal to improve in the next phase.
              </p>
              <br>
              <b>Result shown below.</b>


          </tr>

          <tr>
            <td style="padding:20px;width:100%;text-align:left;">
              <img src="data/cocoLayers.gif" alt="paper photo" style="float:right;width:46.5%;max-width:46.5%;padding:2px">
              <img src="data/cocoLayersFlight.gif" alt="paper photo" style="float:right;width:50%;max-width:50%;padding:2px">
            </td>
          </tr>

        </tbody></table>	

        <hr style="width:50%">


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Self-supervised Frameworks : Kaggle Implementations</heading>
            </td>		
          </tr>	


        </tbody></table>	
      </td>
      </tr>

      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">          
          <p>
            In my free time I have the habit of contributing to the open source community by writing Kaggle notebooks from a mathematical point of view for the latest 
            ongoing research in the field of self supervised learning. My notebooks are a step by step implementation of the latest frameworks on small scale 
            datasets along with links to other research papers that might aid fellow researchers in their problem statements. Some of my implementations are as 
            follows:
          </p>
        </td>	
      </tr>	
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href = "https://www.kaggle.com/code/vedantj/byol-tensorflow-2-0"><heading>BYOL</heading></a>
          <img src="data/byol.png" alt="paper photo" style="width:100%;max-width:100%;">
        </td>		
      </tr>	

      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href = "https://www.kaggle.com/code/vedantj/simclrv1-tensorflow-2-0"><heading>SimCLR</heading></a>
          <img src="data/simCLR.png" alt="paper photo" style="width:100%;max-width:100%;">
        </td>		
      </tr>	

      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href = "https://www.kaggle.com/code/vedantj/scarf-tensorflow-2-0"><heading>SCARF</heading></a>
          <img src="data/scarf.png" alt="paper photo" style="width:100%;max-width:100%;">
        </td>		
      </tr>	

      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href = "https://www.kaggle.com/code/vedantj/k-means-clustering-for-feature-visualisation"><heading>Deep K-Means</heading></a>
          <img src="data/deepKmeans.png" alt="paper photo" style="width:100%;max-width:100%;">
        </td>		
      </tr>	

      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href = "https://www.kaggle.com/code/vedantj/selective-search/notebook"><heading>Selective Search</heading></a>
          <img src="data/ss_pic.png" alt="paper photo" style="width:100%;max-width:100%;">
        </td>		
      </tr>	
  
    </tbody></table>	
    <hr style="width:50%">
</body>

</html>
