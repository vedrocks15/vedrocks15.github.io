<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Vedant S. Joshi</title>
  
  <meta name="author" content="Vedant S. Joshi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vedant S. Joshi</name>
              </p>
              <p>Hi, I am a Research Data Scientist at <a href="https://www.vedantu.com">Vedantu</a>, where my job is to come up with robust A.I. based solutions 
                that are label as well as resource efficient in nature. My work covers the domain of vision, natural language & user centric tabular data originating in 
                the EdTech sector. The goal of my research at Vedantu under <a href="https://scholar.google.com/citations?user=ZFDbLisAAAAJ&hl=en">Dr. Sivanagaraja Tatinati</a> is to 
                leverage the true power of self-supervision to build more accurate models that cater to the needs of students & company stakeholders. 
                <br>
                <br>

                I have completed my B.Tech<b>(Hons.)</b> degree from <a href="https://www.iiitkottayam.ac.in/#!/home">IIIT Kottayam </a> in Computer Science & Engineering where 
                I completed 2 thesis projects under <a href="https://www.iiitkottayam.ac.in/#!/faculty/ebin">Dr. Ebin Deni Raj</a>. The projects covered the topic of 
                attention based lip reading by 3D Conv-Nets (<b>in association with <a href="https://www.tcs.com/tcs-pace/features/tcs-rapid-labs">TCS Rapid Labs</a></b>) 
                & edge based perception module for autonomous fruit pickers.
              </p>

              <p style="text-align:center">
                <a href="mailto:vedrocks15@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/phdVedantCV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/Resume_vedant_joshi.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://github.com/vedrocks15/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/vedant-joshi-b822bb169/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://www.kaggle.com/vedantj/code/">Kaggle</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/vedant.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/vedant.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <br>
        <hr style="width:50%">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                <b>How to make machines think ? </b> <br>
                Is the question that drives my research every morning. 
                Based on this question, my fundamental goal is to understand the inner workings of deep models & make a sincere contribution 
                towards building dynamic mechanisms as well as pathways that match the level of human intelligence by efficiently 
                representing a multitude of input modalities into a single structured latent space that can handle diverse scenarios.
                <br>
                <br>
                <b>Areas of Interest :</b>
              <ul>
                <li>Representational learning</li>
                <li>Self-supervised learning</li>
                <li>Building multi-modal latent spaces</li>
                <li>Ensemble learning</li>
                <li>Large scale data retrieval</li>
                <li>Model quantization & pruning</li>
                <li> Model explainability & interpretability</li>
              </ul>

              </p>
            </td>
          </tr>
        </tbody></table>
        <hr style="width:50%">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Looking For A Match</heading><br>
                <subheading>Self-supervised Clustering For Automatic Doubt Matching In e-learning Platforms</subheading>
              <br>
              <br>
              <strong>Vedant S. Joshi</strong>,
              <a href="https://scholar.google.com/citations?user=ZFDbLisAAAAJ&hl=en">Dr. Sivanagaraja Tatinati</a>,
							<a href="https://scholar.google.co.in/citations?user=bXXM7rMAAAAJ&hl=en">Dr. Yubo Wang</a> | 
              <a href="https://arxiv.org/abs/2208.09600">PrePrint arXiv</a>, 2022
              <br>
              
              <p>
                Doubts are a natural outcome of any student's learning journey & solving them immediately becomes imperative for any EdTech platform so that the steady
                growth of learning for every child can be maintained. In this era of <b>Big Data</b> the volume of data being generated on Vedantu is huge & this is 
                also applicable for the doubts being asked on our platform. The traditional way of solving doubts by <b>Subject Matter Experts</b>(SME) is time consuming, redundant
                & infeasible in nature. Assigning one SME per doubt for every student is unimaginable. Therefore coming up with a system that can find a possible solved match for an asked doubt & create clusters of semantically similar doubts so that
                the redundancy in answering the same question by SMEs is reduced, would help our platform immensely.    
              </p>

              <img src="images/paperDiags.png" alt="paper diagrams" style="width:100%;max-width:100%">


              <p>
                In this work we solved a sub-problem of diagram based matching for doubt questions. For the non-diagram questions we rely on the power of OCRs & transformers
                to build strong text matching engines but for pure <b>reverse image search</b> we come-up with a diagram extraction & matching module which allowed us to prevent 
                re-answering of the same question & reduce the redundancy of diagram based doubts from <b>9.5 lakh individual points</b> to <b>2.5 lakh clusters</b> 
                in our database. We also use the SOTA, self supervised framework BYOL in this project because there are a lot of similar diagram images in our training set & utilising
                the <b>instance discrimination</b> objective in <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> & <a href="https://arxiv.org/abs/1911.05722">MoCo</a> for 
                negative pairs had a high chance of pushing away the latent representations of 2 semantically relevant diagram images thereby affecting our top-k search scores.
              </p>
            </td>	
          </tr>	
          <tr>
            <td style="padding:20px;width:100%;text-align:left;">
              <image src="images/paperResults.png" style="float:right;width:50%;max-width:50%" alt="paper res">
                <p>
                  Our implementation is an improved version of the 
                  <a href="https://arxiv.org/abs/2006.07733">BYOL</a> which is trained on diagram images extracted from a 
                  custom trained <a href="https://arxiv.org/abs/2011.08036">Scaled YOLOv4</a> module that gave a mean average 
                  precision of <b>90%</b> for diagram detection. The main contribution of our work was to come up with a new, domain specific augmentation
                  pipeline that modelled the possible noises generated by a student while uploading an image on our platform. The newly designed augmentations were
                  guided by <b>mutual information</b> metrics to capture flashes, skews, random camera noise etc. but at the same time not loose semantic relevance of the 
                  matching pairs. The augmentations played a crucial role 
                  towards learning of noise invariant, compressed representations of diagrams & aided in achieving more accurate as well as relevant matches for a given 
                  input query.
                </p>
                <p>
                  The diagrams on the right show the improved convergance of our models along with stronger clustering abilities along the diagonals of <b>image 
                  similarity matrices</b>. In order to deploy the model, the vectors computed by our <b>Custom BYOL</b> were searched using approximate K-NN algorithms such 
                  as <a href="https://arxiv.org/abs/1603.09320">Heirarchial Navigable Small Worlds (HNSW)</a> & the search space was reduced by 
                  performing clustering on the vectors using <a href="https://arxiv.org/abs/1802.03426">UMAP</a> & <a href="https://arxiv.org/abs/1911.02282">HDBSCAN</a>. Each
                  cluster was represented by a centroid which was the key vector in the HNSW search space.
                </p>
            </td>
          </tr>
        </tbody></table>	
        <hr style="width:50%">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>For Your Eyes Only</heading><br>
                <subheading>Character Level Model For Lip Reading [
                                                                    <a href = "https://mgmits.ac.in/icscc-2020/">
                                                                      <b style='color:red;'>Won Best Paper</b>
                                                                    </a>]</subheading>
              <br>
              <br>
              <strong>Vedant S. Joshi</strong>,
              <a href="https://www.iiitkottayam.ac.in/#!/faculty/ebinn">Dr. Ebin Deni Raj</a> | 
              <a href="https://ieeexplore.ieee.org/document/9528104">2021 8th International Conference on Smart Computing & Communications (ICSCC)</a>
              <br>
              
              <p>
                The amazing ability of our human mind to handle multiple input sources at once & make sense of the environment in which we are present is truly
                extra-ordinary. Along with this ability, our mind is also dynamic enough to adapt to situations where we loose certain input sources & still make the best
                possible use of the information availale to us. We all think that speech understanding is a skill that is entirely dependent on hearing but vision also 
                plays a key hidden role which helps us to disambiguate a lot of confusing scenarios. For people who have trouble in hearing, they rely a lot on their vision to
                understand speech. Based on this observation, I worked on my thesis titled <b> For Your Eyes Only </b>, to build an end to end deep learning 
                system which is able to accurately map a set of lip movements in a given video to its corresponding character.
              </p>
              </td>
              </tr>

              <tr>
                <td style="padding:20px;width:100%;text-align:right;vertical-align: middle;">
                  <image src="images/lipNet.png" style="float:left;width:50%;max-width:50%" alt="left aligned image" class="hoverZoomLink">
                  <p>
                    The system is trained on a subset of words from the large scale <a href = "https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html">Oxford-BBC Lip Reading in the Wild (LRW) Dataset</a>.
                    The whole problem statement is formulated around single word prediction, character by character because the aim of our thesis was to come up with a system that
                    could learn lip movement to character mapping using only a limited set of words in training. This approach of <b>character level prediction</b>
                    made our system more generalized in nature, incase we encountered a previously unseen word. The out of vocabulary words cannot be handled by simple classification models.
                  </p>
                  <p>
                    We make use of <b>112 words</b> & the videos for each of these words are processed according to the timestamp at which they are spoken. To further 
                    improve the model performance, we make use of <a href = "https://pypi.org/project/dlib/">D-Lib facial features library</a> to extract only the 
                    lip region from these videos with some buffer. Also speaking speed normalisation is applied so that every word video would be 22 frames long in 
                    the <b>temporal dimension</b> so that the training data can be efficiently batched into tensors.
                  </p>

                  <p>
                    For baseline performance we repurpose <a href = "https://arxiv.org/abs/1611.01599">DeepMind's LipNet model</a> from sentence level to word level predictor & train 
                    it from scratch on the LRW subset. To learn further complex frame interactions & improve model explainability, we add 
                    <a href="https://arxiv.org/abs/1409.0473">Bahdanau attention mechanism</a> in between the the 3D ConvNet encoder & GRU decoder which 
                    is depicted by the heatmap on the left. The final prediction is made by using <b>greedy CTC decoding techniques</b> in order to get a single character that was
                    scaled to multiple frames during the alignment learning. 
                    <br>
                    <br>
                    <b>Result shown below.</b>
                  </p>
                </td>
              </tr>	


              <tr>
                <td>
                  <image src="data/lipReadingVid.gif" style="float:right;width:100%;max-width:100%" alt="right aligned image" class="hoverZoomLink">
                </td>
              </tr>	

        </tbody></table>	
        <br>
        <hr style="width:50%">
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Quantized Coconut Detection Models with  Edge Devices</heading>
              <br>
              <br>
              <strong>Vedant S. Joshi</strong>,
              <a href="https://www.iiitkottayam.ac.in/#!/faculty/ebinn">Dr. Ebin Deni Raj</a>,
							<a href="https://scholar.google.com/citations?user=1G2WBksAAAAJ&hl=en">Jeena Thomas</a> | 
              <a href="hhttps://www.worldscientific.com/doi/abs/10.1142/S0219265921440102">Journal of Interconnection Networks</a>, 2022        
            </td>		
          </tr>	
          
          <tr>
            <td style="padding:20px;width:100%;text-align:left;vertical-align: middle;">
              <image src="data/cocoLayersModel.png" style="float:right;width:50%;max-width:50%" alt="right aligned image" class="hoverZoomLink">
              <p>
              Owing to the landscape & natural conditions, <b>coconut</b> is an important fruit in Kerala. Coconut farms are an important aspect
              of the state's economy therefore a lot of citizen's livelihood is dependent on this fruit. Plucking coconuts from plam trees is a huge 
              challenge since it involves scaling large heights without proper equipment which could lead to serious injuries for the daily wage
              workers. Inspired by this, I started to work on my Honours degree thesis project <b>Coco-Layers</b>.
              </p>
              <p>
                The whole project was divided into 2 modules : 
                <ul>
                  <li><b>Hardware :</b> Building of an autonomous drone with flight stabilization for curation of a novel
                    coconuts dataset which captured variances in real world lighting conditions, multiple scales at which coconuts occur in wild & 
                    possible hindrances that could confuse the overall detection system.</li>
                    <br>
                  <li><b>Software : </b> On board object detecion module that supported edge based real time inference without loosing detection accuracy.</li>
                </ul>

              We experimented on <b>Nvidia Jetson Nano</b> & <b>Raspberry Pi 3b+</b> using <b>Nvidia tensor RT</b> and <b>Tensorflow lite quantisation</b> kits for reducing model resource
              consumption. To further push the limits, a frame buffer handling mechanism was written in OpenCV to reduce the calls to main memory for achieving faster frame
              processing rates. The peak accuracy achieved in the study was <b>0.4 mean average precision</b> with a <b>22 FPS</b> detection rate 
              (using a <a href="https://arxiv.org/abs/2004.10934">Tiny YOLOv4</a>) in real time flight. The downside
              of fast processing was reduced flight time from <b>12 minutes to 6 minutes</b> which is kept as a goal to improve in the next phase.
              </p>
              <br>
              <b>Result shown below.</b>


          </tr>

          <tr>
            <td style="padding:20px;width:100%;text-align:left;">
              <img src="data/cocoLayers.gif" alt="paper photo" style="float:right;width:46.5%;max-width:46.5%;padding:2px">
              <img src="data/cocoLayersFlight.gif" alt="paper photo" style="float:right;width:50%;max-width:50%;padding:2px">
            </td>
          </tr>

        </tbody></table>	

        <hr style="width:50%">


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Self-supervised Frameworks : Kaggle Implementations</heading>
            </td>		
          </tr>	


        </tbody></table>	
      </td>
      </tr>

      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">          
          <p>
            In my free time I have the habit of contributing to the open source community by writing Kaggle notebooks from a mathematical point of view for the latest 
            ongoing research in the field of self supervised learning. My notebooks are a step by step implementation of the latest frameworks on small scale 
            datasets along with links to other research papers that might aid fellow researchers in their problem statements.
          </p>
        </td>	
      </tr>	
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href = "https://www.kaggle.com/code/vedantj/simclrv1-tensorflow-2-0"><heading>SimCLR</heading></a>
          <img src="data/simCLR.png" alt="paper photo" style="width:100%;max-width:100%;">
        </td>		
      </tr>	

      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href = "https://www.kaggle.com/code/vedantj/scarf-tensorflow-2-0"><heading>SCARF</heading></a>
          <img src="data/scarf.png" alt="paper photo" style="width:100%;max-width:100%;">
        </td>		
      </tr>	

      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href = "https://www.kaggle.com/code/vedantj/k-means-clustering-for-feature-visualisation"><heading>Deep K-Means</heading></a>
          <img src="data/deepKmeans.png" alt="paper photo" style="width:100%;max-width:100%;">
        </td>		
      </tr>	
  
    </tbody></table>	
    <hr style="width:50%">
</body>

</html>
